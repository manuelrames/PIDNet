{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78d4558-279b-4de3-a55e-a35ec55080fc",
   "metadata": {},
   "source": [
    "# PIDNet Training on Railways SemSegm in AzureML Pipeline\n",
    "\n",
    "## 0. Connect to your workspace\n",
    "Connect to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8361eb7f-4719-4679-87eb-73c3799f87c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.48.0 to work with aml-con-fer-cvrail-euwe-dev\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16634147-c21a-43eb-9de6-3ff430ec7000",
   "metadata": {},
   "source": [
    "## 1. Create scripts for pipeline steps\n",
    "First, let's create a folder for the script files we'll use in the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6b7087-c984-40a4-98df-b139ecc685bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pidnet_experiment_folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "pidnet_experiment_folder = 'pidnet_experiment_folder'\n",
    "os.makedirs(pidnet_experiment_folder, exist_ok=True)\n",
    "\n",
    "print(pidnet_experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558835ea-3bfc-4c09-bc51-5ed561d4f396",
   "metadata": {},
   "source": [
    "### 1.1. Copy necessary files into experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69be55a1-243f-4d04-851a-dacccf6d0c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pidnet_experiment_folder/pretrained_models/imagenet/PIDNet_M_ImageNet.pth.tar'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# configs folder\n",
    "configs_path = os.path.join(pidnet_experiment_folder, 'configs')\n",
    "os.makedirs(configs_path, exist_ok=True)\n",
    "shutil.copy('../configs/__init__.py', configs_path)\n",
    "shutil.copy('../configs/default.py', configs_path)\n",
    "#shutil.copytree('../configs/railways', configs_path + '/railways', dirs_exist_ok=True)\n",
    "shutil.copytree('../configs/railways_5classes', configs_path + '/railways_5classes', dirs_exist_ok=True)\n",
    "\n",
    "# utils\n",
    "utils_path = os.path.join(pidnet_experiment_folder, 'utils')\n",
    "shutil.copytree('../utils', utils_path, dirs_exist_ok=True)\n",
    "\n",
    "# tools\n",
    "tools_path = os.path.join(pidnet_experiment_folder, 'tools')\n",
    "shutil.copytree('../tools', tools_path, dirs_exist_ok=True)\n",
    "\n",
    "# models\n",
    "models_path = os.path.join(pidnet_experiment_folder, 'models')\n",
    "shutil.copytree('../models', models_path, dirs_exist_ok=True)\n",
    "\n",
    "# datasets\n",
    "datasets_path = os.path.join(pidnet_experiment_folder, 'datasets')\n",
    "shutil.copytree('../datasets', datasets_path, dirs_exist_ok=True)\n",
    "\n",
    "# pretrained models\n",
    "pretrained_models_path = os.path.join(pidnet_experiment_folder, 'pretrained_models/imagenet')\n",
    "os.makedirs(pretrained_models_path, exist_ok=True)\n",
    "#shutil.copy('../pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar', pretrained_models_path)\n",
    "shutil.copy('../pretrained_models/imagenet/PIDNet_M_ImageNet.pth.tar', pretrained_models_path)\n",
    "#shutil.copy('../pretrained_models/imagenet/PIDNet_L_ImageNet.pth.tar', pretrained_models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b0a74-b830-4630-a9b3-2d45449bf5c3",
   "metadata": {},
   "source": [
    "## 2. Prepare a compute environment for the pipeline steps\n",
    "### 2.1. Environment Setup\n",
    "Import the environment that will be used to run the experiment. This method of execution avoids dependencies on updates from Azure machines, as it has been generated from Docker image.\n",
    "\n",
    "If you need additional packages you can register your environment and run the experiment with it. To avoid Azure VM updates it is strongly recommended to register the environment with a pre-compiled Docker image. Below is an example of how to register your environment in this way:\n",
    "\n",
    "#### 2.1.1. Training Step\n",
    "First, the compute target set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae52060c-a0af-49ce-b310-ad06a8dd3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"mrc-railways-nc6\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', max_nodes=4)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24befe10-4cfa-44d5-8669-abeda8fb6f51",
   "metadata": {},
   "source": [
    "Then the compute environment based on a Dockerfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a64aa62-d4f8-400f-a3a1-f786f427d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/westeurope/workspaces/3147bafb-f565-4a86-b7c2-e84fb1ba22c4/environments/railways_pidnet_training_env/versions/7\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": \"\\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20211221.v1\\n\\nENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/pidnet\\n\\n# Create conda environment\\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\\\\n   python=3.9 pip=22.1.2\\n\\n# Prepend path to AzureML conda environment\\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\\n\\n# Solve pub key problem (https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772)\\nRUN apt-key del 7fa2af80\\nRUN apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\\n\\n# Solve opencv dependencies\\nRUN apt-get update\\nRUN apt-get install ffmpeg libsm6 libxext6 -y\\n\\n# Install Pytorch dependencies\\nRUN pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\\n\\n# Install pip dependencies\\nRUN pip install 'azureml-core' \\\\\\n                'azureml-dataprep' \\\\\\n                'azureml-defaults' \\\\\\n                'azureml-pipeline' \\\\\\n                'azureml-pipeline-core ' \\\\\\n                'azureml-pipeline-steps' \\\\\\n                'azureml-telemetry '\\n\\n# Install pip dependencies\\nRUN pip install 'EasyDict==1.7' \\\\\\n                'shapely' \\\\\\n                'Cython' \\\\\\n                'scipy' \\\\\\n                'pandas' \\\\\\n                'pyyaml' \\\\\\n                'json_tricks' \\\\\\n                'scikit-image' \\\\\\n                'yacs>=0.1.5' \\\\\\n                'tensorboardX>=1.6' \\\\\\n                'tqdm' \\\\\\n                'ninja' \\\\\\n                'opencv-python'\\n\\n# This is needed for mpi to locate libpython\\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\\n\",\n",
       "        \"baseImage\": null,\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": true,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"railways_pidnet_training_env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8.13\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"project_environment\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": true\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"7\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Create a Python environment for the experiment (from Dockerfile)\n",
    "pidnet_env = Environment(\"railways_pidnet_training_env\")\n",
    "\n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20211221.v1\n",
    "\n",
    "ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/pidnet\n",
    "\n",
    "# Create conda environment\n",
    "RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n",
    "   python=3.9 pip=22.1.2\n",
    "\n",
    "# Prepend path to AzureML conda environment\n",
    "ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\n",
    "\n",
    "# Solve pub key problem (https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772)\n",
    "RUN apt-key del 7fa2af80\n",
    "RUN apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n",
    "\n",
    "# Solve opencv dependencies\n",
    "RUN apt-get update\n",
    "RUN apt-get install ffmpeg libsm6 libxext6 -y\n",
    "\n",
    "# Install Pytorch dependencies\n",
    "RUN pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Install pip dependencies\n",
    "RUN pip install 'azureml-core' \\\n",
    "                'azureml-dataprep' \\\n",
    "                'azureml-defaults' \\\n",
    "                'azureml-pipeline' \\\n",
    "                'azureml-pipeline-core ' \\\n",
    "                'azureml-pipeline-steps' \\\n",
    "                'azureml-telemetry '\n",
    "\n",
    "# Install pip dependencies\n",
    "RUN pip install 'EasyDict==1.7' \\\n",
    "                'shapely' \\\n",
    "                'Cython' \\\n",
    "                'scipy' \\\n",
    "                'pandas' \\\n",
    "                'pyyaml' \\\n",
    "                'json_tricks' \\\n",
    "                'scikit-image' \\\n",
    "                'yacs>=0.1.5' \\\n",
    "                'tensorboardX>=1.6' \\\n",
    "                'tqdm' \\\n",
    "                'ninja' \\\n",
    "                'opencv-python'\n",
    "\n",
    "# This is needed for mpi to locate libpython\n",
    "ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\n",
    "\"\"\"\n",
    "\n",
    "pidnet_env.docker.enabled = True\n",
    "pidnet_env.docker.base_image = None\n",
    "pidnet_env.python.user_managed_dependencies = True\n",
    "pidnet_env.docker.base_dockerfile = dockerfile\n",
    "pidnet_env.save_to_directory(path=\"./railways_pidnet_training_env\", overwrite=True)\n",
    "# Register the environment\n",
    "pidnet_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70be095c-ac82-4af2-bb7d-eb12cefeb227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIDNet run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Get the environment \n",
    "pidnet_env = Environment.get(workspace=ws,name=\"railways_pidnet_training_env\")\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pidnet_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pidnet_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pidnet_run_config.environment = pidnet_env\n",
    "\n",
    "print (\"PIDNet run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24523d8-e24f-47f5-9466-ab6dfbbbbd31",
   "metadata": {},
   "source": [
    "## 4. Create and run a pipeline\n",
    "Now you're ready to create and run a pipeline.\n",
    "\n",
    "First you need to define the steps for the pipeline, and any data references that need to be passed between them. In this case, the first step must write the prepared data to a folder that can be read from by the second step. Since the steps will be run on remote compute (and in fact, could each be run on different compute), the folder path must be passed as a data reference to a location in a datastore within the workspace. The OutputFileDatasetConfig object is a special kind of data reference that is used for interim storage locations that can be passed between pipeline steps, so you'll create one and use at as the output for the first step and the input for the second step. Note that you need to pass it as a script argument so your code can access the datastore location referenced by the data reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3881dea-196d-4248-a4cd-3f0204e3578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core import Dataset, ScriptRunConfig\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice, uniform, quniform, MedianStoppingPolicy\n",
    "\n",
    "# Get the training dataset\n",
    "training_ds = Dataset.get_by_name(ws, name='railways_semsegm_5classes_dataset')\n",
    "\n",
    "# Step 1, Detect the solar panel defects on thermal images\n",
    "#pidnet_training_step = PythonScriptStep(name = \"Railways SemSegm PIDNet Training\",\n",
    "#                                source_directory = pidnet_experiment_folder,\n",
    "#                                script_name = \"./tools/train_azureml.py\",\n",
    "#                                arguments = ['--cfg', 'configs/railways/pidnet_medium_railways.yaml',\n",
    "#                                            '--training-dataset', training_ds.as_named_input('railways_semsegm_dataset')],\n",
    "#                                compute_target = pipeline_cluster,\n",
    "#                                runconfig = pidnet_run_config,\n",
    "#                                allow_reuse = True)\n",
    "\n",
    "pidnet_training_step = ScriptRunConfig(source_directory = pidnet_experiment_folder,\n",
    "                                script = \"./tools/train_azureml.py\",\n",
    "                                arguments = ['--cfg', 'configs/railways_5classes/pidnet_medium_railways_5classes.yaml',\n",
    "                                            '--training-dataset', training_ds.as_named_input('railways_semsegm_5classes_dataset')],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                environment = pidnet_env)\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(8, 12, 16),\n",
    "        '--learning-rate' : uniform(0.001, 0.01),\n",
    "        '--max-num-epochs' : quniform(100, 150, 1),\n",
    "        '--momentum' : uniform(0.85, 0.95),\n",
    "        '--weight-decay' : uniform(0.00005, 0.005)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Establish an early termination policy to save training costs\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=10)\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(run_config=pidnet_training_step, \n",
    "                          hyperparameter_sampling=params, \n",
    "                          policy=early_termination_policy, # No early stopping policy\n",
    "                          primary_metric_name='mean_IoU', # Find the highest mean_IoU metric\n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=30, # Restict the experiment to 6 iterations\n",
    "                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a531e21d-22cf-4267-aa27-bf9bace21636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n",
    "#run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "#RunDetails(run).show()\n",
    "#run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96c8fe-b3ae-4a50-a5e9-8853ccf37ec9",
   "metadata": {},
   "source": [
    "OK, you're ready build the pipeline from the steps you've defined and run it as an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500f03d-19f2-4652-942b-4f31cdd32c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    \n",
      "Area                             Computer Vision, Railway Inspection\n",
      "Target                                 Railway Semantic Segmentation\n",
      "Subscription ID                 764a3c74-3d65-4a9c-bc90-661b00666572\n",
      "Workspace                                aml-con-fer-cvrail-euwe-dev\n",
      "Resource Group                            rg-con-fer-cvrail-euwe-dev\n",
      "Location                                                  westeurope\n",
      "ADL                                      andres.santos@ferrovial.com\n",
      "Company                                                  Corporacion\n",
      "DevOps                                                CON-FER-CVRAIL\n",
      "Environment                                              Development\n",
      "OU                                                     Ferrocarriles\n",
      "Project                                           Railway Inspection\n",
      "Responsible                                 mcastrillo@ferrovial.com\n",
      "area                                              railway inspection\n",
      "type             railway semantic segmentation PIDNet model training\n",
      "Pipeline submitted for execution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf86337b8154042ad3a3e4572e53891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/HD_c4fabe37-8835-473c-8647-824413410f00?wsid=/subscriptions/764a3c74-3d65-4a9c-bc90-661b00666572/resourcegroups/rg-con-fer-cvrail-euwe-dev/workspaces/aml-con-fer-cvrail-euwe-dev&tid=a9a8e375-fac1-4ec2-820a-cfb6eb5cf01b\", \"run_id\": \"HD_c4fabe37-8835-473c-8647-824413410f00\", \"run_properties\": {\"run_id\": \"HD_c4fabe37-8835-473c-8647-824413410f00\", \"created_utc\": \"2023-01-19T16:39:04.736205Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\":\\\"mean_IoU\\\",\\\"goal\\\":\\\"maximize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"71cc5d40-e373-4a51-b5f9-2f8c2c590940\", \"user_agent\": \"python/3.9.16 (Linux-5.15.0-58-generic-x86_64-with-glibc2.31) msrest/0.7.1 Hyperdrive.Service/1.0.0 Hyperdrive.SDK/core.1.48.0\", \"space_size\": \"infinite_space_size\"}, \"tags\": {\"_aml_system_max_concurrent_jobs\": \"2\", \"_aml_system_max_total_jobs\": \"30\", \"_aml_system_max_duration_minutes\": \"10080\", \"_aml_system_policy_config\": \"{\\\"name\\\":\\\"MedianStopping\\\",\\\"properties\\\":{\\\"evaluation_interval\\\":1,\\\"delay_evaluation\\\":10}}\", \"_aml_system_generator_config\": \"{\\\"name\\\":\\\"RANDOM\\\",\\\"parameter_space\\\":{\\\"--batch-size\\\":[\\\"choice\\\",[[8,12,16]]],\\\"--learning-rate\\\":[\\\"uniform\\\",[0.001,0.01]],\\\"--max-num-epochs\\\":[\\\"quniform\\\",[100,150,1]],\\\"--momentum\\\":[\\\"uniform\\\",[0.85,0.95]],\\\"--weight-decay\\\":[\\\"uniform\\\",[5E-05,0.005]]},\\\"properties\\\":null}\", \"_aml_system_primary_metric_config\": \"{\\\"name\\\":\\\"mean_IoU\\\",\\\"goal\\\":\\\"maximize\\\"}\", \"_aml_system_platform_config\": \"{\\\"ServiceAddress\\\":\\\"https://westeurope.experiments.azureml.net\\\",\\\"SubscriptionId\\\":\\\"764a3c74-3d65-4a9c-bc90-661b00666572\\\",\\\"ResourceGroupName\\\":\\\"rg-con-fer-cvrail-euwe-dev\\\",\\\"WorkspaceName\\\":\\\"aml-con-fer-cvrail-euwe-dev\\\",\\\"ExperimentName\\\":\\\"railway-pidnet-model-training\\\",\\\"Definition\\\":{\\\"Configuration\\\":null,\\\"Attribution\\\":null,\\\"TelemetryValues\\\":{\\\"amlClientType\\\":\\\"azureml-sdk-train\\\",\\\"amlClientModule\\\":\\\"[Scrubbed]\\\",\\\"amlClientFunction\\\":\\\"[Scrubbed]\\\",\\\"tenantId\\\":\\\"a9a8e375-fac1-4ec2-820a-cfb6eb5cf01b\\\",\\\"amlClientRequestId\\\":\\\"f64fef8d-7e6f-46ea-9b03-5354dd96bf93\\\",\\\"amlClientSessionId\\\":\\\"66492e62-bc60-4bc5-aa95-a4117963f4a8\\\",\\\"subscriptionId\\\":\\\"764a3c74-3d65-4a9c-bc90-661b00666572\\\",\\\"estimator\\\":\\\"NoneType\\\",\\\"samplingMethod\\\":\\\"RANDOM\\\",\\\"terminationPolicy\\\":\\\"MedianStopping\\\",\\\"primaryMetricGoal\\\":\\\"maximize\\\",\\\"maxTotalRuns\\\":30,\\\"maxConcurrentRuns\\\":2,\\\"maxDurationMinutes\\\":10080,\\\"vmSize\\\":null},\\\"Overrides\\\":{\\\"Script\\\":\\\"./tools/train_azureml.py\\\",\\\"Command\\\":\\\"\\\",\\\"UseAbsolutePath\\\":false,\\\"Arguments\\\":[\\\"--cfg\\\",\\\"configs/railways_5classes/pidnet_medium_railways_5classes.yaml\\\",\\\"--training-dataset\\\",\\\"DatasetConsumptionConfig:railways_semsegm_5classes_dataset\\\"],\\\"SourceDirectoryDataStore\\\":null,\\\"Framework\\\":0,\\\"Communicator\\\":0,\\\"Target\\\":\\\"mrc-railways-nc6\\\",\\\"DataReferences\\\":{},\\\"Data\\\":{\\\"railways_semsegm_5classes_dataset\\\":{\\\"DataLocation\\\":{\\\"Dataset\\\":{\\\"Id\\\":\\\"936eb3c1-0371-447e-b103-23cc2c0e5f5c\\\",\\\"Name\\\":\\\"railways_semsegm_5classes_dataset\\\",\\\"Version\\\":\\\"1\\\"},\\\"DataPath\\\":null,\\\"Uri\\\":null,\\\"Type\\\":null},\\\"Mechanism\\\":\\\"Direct\\\",\\\"EnvironmentVariableName\\\":\\\"railways_semsegm_5classes_dataset\\\",\\\"PathOnCompute\\\":null,\\\"Overwrite\\\":false,\\\"Options\\\":null}},\\\"OutputData\\\":{},\\\"Datacaches\\\":[],\\\"JobName\\\":null,\\\"MaxRunDurationSeconds\\\":2592000,\\\"NodeCount\\\":1,\\\"InstanceTypes\\\":[],\\\"Priority\\\":null,\\\"CredentialPassthrough\\\":false,\\\"Identity\\\":null,\\\"Environment\\\":{\\\"Name\\\":\\\"railways_pidnet_training_env\\\",\\\"Version\\\":\\\"7\\\",\\\"AssetId\\\":\\\"azureml://locations/westeurope/workspaces/3147bafb-f565-4a86-b7c2-e84fb1ba22c4/environments/railways_pidnet_training_env/versions/7\\\",\\\"AutoRebuild\\\":true,\\\"Python\\\":{\\\"InterpreterPath\\\":\\\"python\\\",\\\"UserManagedDependencies\\\":true,\\\"CondaDependencies\\\":{\\\"name\\\":\\\"project_environment\\\",\\\"dependencies\\\":[\\\"python=3.8.13\\\",{\\\"pip\\\":[\\\"azureml-defaults\\\"]}],\\\"channels\\\":[\\\"anaconda\\\",\\\"conda-forge\\\"]},\\\"BaseCondaEnvironment\\\":null},\\\"EnvironmentVariables\\\":{\\\"EXAMPLE_ENV_VAR\\\":\\\"EXAMPLE_VALUE\\\"},\\\"Docker\\\":{\\\"BaseImage\\\":null,\\\"Platform\\\":{\\\"Os\\\":\\\"Linux\\\",\\\"Architecture\\\":\\\"amd64\\\"},\\\"BaseDockerfile\\\":\\\"\\\\nFROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20211221.v1\\\\n\\\\nENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/pidnet\\\\n\\\\n# Create conda environment\\\\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\\\\\\\\\\n   python=3.9 pip=22.1.2\\\\n\\\\n# Prepend path to AzureML conda environment\\\\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\\\\n\\\\n# Solve pub key problem (https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772)\\\\nRUN apt-key del 7fa2af80\\\\nRUN apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\\\\n\\\\n# Solve opencv dependencies\\\\nRUN apt-get update\\\\nRUN apt-get install ffmpeg libsm6 libxext6 -y\\\\n\\\\n# Install Pytorch dependencies\\\\nRUN pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\\\\n\\\\n# Install pip dependencies\\\\nRUN pip install 'azureml-core' \\\\\\\\\\\\n                'azureml-dataprep' \\\\\\\\\\\\n                'azureml-defaults' \\\\\\\\\\\\n                'azureml-pipeline' \\\\\\\\\\\\n                'azureml-pipeline-core ' \\\\\\\\\\\\n                'azureml-pipeline-steps' \\\\\\\\\\\\n                'azureml-telemetry '\\\\n\\\\n# Install pip dependencies\\\\nRUN pip install 'EasyDict==1.7' \\\\\\\\\\\\n                'shapely' \\\\\\\\\\\\n                'Cython' \\\\\\\\\\\\n                'scipy' \\\\\\\\\\\\n                'pandas' \\\\\\\\\\\\n                'pyyaml' \\\\\\\\\\\\n                'json_tricks' \\\\\\\\\\\\n                'scikit-image' \\\\\\\\\\\\n                'yacs>=0.1.5' \\\\\\\\\\\\n                'tensorboardX>=1.6' \\\\\\\\\\\\n                'tqdm' \\\\\\\\\\\\n                'ninja' \\\\\\\\\\\\n                'opencv-python'\\\\n\\\\n# This is needed for mpi to locate libpython\\\\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\\\\n\\\",\\\"BaseImageRegistry\\\":{\\\"Address\\\":null,\\\"Username\\\":null,\\\"Password\\\":null},\\\"Enabled\\\":true,\\\"Arguments\\\":[]},\\\"Spark\\\":{\\\"Repositories\\\":[],\\\"Packages\\\":[],\\\"PrecachePackages\\\":true},\\\"InferencingStackVersion\\\":null},\\\"History\\\":{\\\"OutputCollection\\\":true,\\\"DirectoriesToWatch\\\":[\\\"logs\\\"],\\\"EnableMLflowTracking\\\":true,\\\"snapshotProject\\\":true},\\\"Spark\\\":{\\\"Configuration\\\":{\\\"spark.app.name\\\":\\\"Azure ML Experiment\\\",\\\"spark.yarn.maxAppAttempts\\\":\\\"1\\\"}},\\\"ParallelTask\\\":{\\\"MaxRetriesPerWorker\\\":0,\\\"WorkerCountPerNode\\\":1,\\\"TerminalExitCodes\\\":null,\\\"Configuration\\\":{}},\\\"BatchAi\\\":{\\\"NodeCount\\\":0},\\\"AmlCompute\\\":{\\\"Name\\\":null,\\\"VmSize\\\":null,\\\"RetainCluster\\\":false,\\\"ClusterMaxNodeCount\\\":null},\\\"AISuperComputer\\\":{\\\"InstanceType\\\":\\\"D2\\\",\\\"FrameworkImage\\\":null,\\\"ImageVersion\\\":null,\\\"Location\\\":null,\\\"AISuperComputerStorageData\\\":null,\\\"Interactive\\\":false,\\\"ScalePolicy\\\":null,\\\"VirtualClusterArmId\\\":null,\\\"TensorboardLogDirectory\\\":null,\\\"SSHPublicKey\\\":null,\\\"SSHPublicKeys\\\":null,\\\"EnableAzmlInt\\\":true,\\\"Priority\\\":\\\"Medium\\\",\\\"SLATier\\\":\\\"Standard\\\",\\\"UserAlias\\\":null},\\\"KubernetesCompute\\\":{\\\"InstanceType\\\":null},\\\"Tensorflow\\\":{\\\"WorkerCount\\\":1,\\\"ParameterServerCount\\\":1},\\\"Mpi\\\":{\\\"ProcessCountPerNode\\\":1},\\\"PyTorch\\\":{\\\"CommunicationBackend\\\":\\\"nccl\\\",\\\"ProcessCount\\\":null},\\\"Hdi\\\":{\\\"YarnDeployMode\\\":2},\\\"ContainerInstance\\\":{\\\"Region\\\":null,\\\"CpuCores\\\":2.0,\\\"MemoryGb\\\":3.5},\\\"ExposedPorts\\\":null,\\\"Docker\\\":{\\\"UseDocker\\\":true,\\\"SharedVolumes\\\":true,\\\"ShmSize\\\":null,\\\"Arguments\\\":[]},\\\"Cmk8sCompute\\\":{\\\"Configuration\\\":{}},\\\"CommandReturnCodeConfig\\\":{\\\"ReturnCode\\\":0,\\\"SuccessfulReturnCodes\\\":[]},\\\"EnvironmentVariables\\\":{},\\\"ApplicationEndpoints\\\":{},\\\"Parameters\\\":[]},\\\"SnapshotId\\\":\\\"71cc5d40-e373-4a51-b5f9-2f8c2c590940\\\",\\\"Snapshots\\\":[],\\\"SourceCodeDataReference\\\":null,\\\"ParentRunId\\\":null,\\\"DataContainerId\\\":null,\\\"RunType\\\":null,\\\"DisplayName\\\":null,\\\"EnvironmentAssetId\\\":null,\\\"Properties\\\":{},\\\"Tags\\\":{},\\\"AggregatedArtifactPath\\\":null},\\\"ParentRunId\\\":\\\"HD_c4fabe37-8835-473c-8647-824413410f00\\\"}\", \"_aml_system_resume_child_runs\": \"null\", \"_aml_system_all_jobs_generated\": \"false\", \"_aml_system_cancellation_requested\": \"false\", \"_aml_system_progress_metadata_evaluation_timestamp\": \"\\\"2023-01-19T16:39:05.369360\\\"\", \"_aml_system_progress_metadata_digest\": \"\\\"4a522d12d7dfc708d08bd74558f43315c45249ff4dbae122ec696262060e1de5\\\"\", \"_aml_system_progress_metadata_active_timestamp\": \"\\\"2023-01-19T16:39:05.369360\\\"\", \"_aml_system_optimizer_state_artifact\": \"null\", \"_aml_system_outdated_optimizer_state_artifacts\": \"\\\"[]\\\"\", \"_aml_system_HD_c4fabe37-8835-473c-8647-824413410f00_0\": \"{\\\"--batch-size\\\": 12, \\\"--learning-rate\\\": 0.00473223742665814, \\\"--max-num-epochs\\\": 146.0, \\\"--momentum\\\": 0.8797986774706478, \\\"--weight-decay\\\": 0.0010074452868463516}\", \"_aml_system_HD_c4fabe37-8835-473c-8647-824413410f00_1\": \"{\\\"--batch-size\\\": 12, \\\"--learning-rate\\\": 0.0023797690296738973, \\\"--max-num-epochs\\\": 145.0, \\\"--momentum\\\": 0.8900625403494643, \\\"--weight-decay\\\": 0.003997022541027771}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://confercvraileuwedevsa.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_c4fabe37-8835-473c-8647-824413410f00/azureml-logs/hyperdrive.txt?sv=2019-07-07&sr=b&sig=7CRif9MNludSR%2Fc%2FIm9NVx7TVP7KE4tUWKttw0wEW4o%3D&skoid=a78d278e-fa8c-45ff-901c-bbbb0ead43d2&sktid=a9a8e375-fac1-4ec2-820a-cfb6eb5cf01b&skt=2023-01-19T10%3A09%3A06Z&ske=2023-01-20T18%3A19%3A06Z&sks=b&skv=2019-07-07&st=2023-01-19T16%3A29%3A07Z&se=2023-01-20T00%3A39%3A07Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:03:05\", \"run_number\": \"1674146344\", \"run_queued_details\": {\"status\": \"Running\", \"details\": null}, \"hyper_parameters\": {\"--batch-size\": [\"choice\", [[8, 12, 16]]], \"--learning-rate\": [\"uniform\", [0.001, 0.01]], \"--max-num-epochs\": [\"quniform\", [100, 150, 1]], \"--momentum\": [\"uniform\", [0.85, 0.95]], \"--weight-decay\": [\"uniform\", [5e-05, 0.005]]}}, \"child_runs\": [{\"run_id\": \"HD_c4fabe37-8835-473c-8647-824413410f00_0\", \"run_number\": 1674146346, \"metric\": null, \"status\": \"Queued\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"\", \"end_time\": \"\", \"created_time\": \"2023-01-19T16:39:06.513386Z\", \"created_time_dt\": \"2023-01-19T16:39:06.513386Z\", \"duration\": \"0:03:05\", \"hyperdrive_id\": \"c4fabe37-8835-473c-8647-824413410f00\", \"arguments\": null, \"param_--batch-size\": 12, \"param_--learning-rate\": 0.00473223742665814, \"param_--max-num-epochs\": 146.0, \"param_--momentum\": 0.8797986774706478, \"param_--weight-decay\": 0.0010074452868463516}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2023-01-19T16:39:05.507582][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\\n[2023-01-19T16:39:06.4306644Z][SCHEDULER][INFO]Scheduling job, id='HD_c4fabe37-8835-473c-8647-824413410f00_1' \\n[2023-01-19T16:39:06.3481073Z][SCHEDULER][INFO]Scheduling job, id='HD_c4fabe37-8835-473c-8647-824413410f00_0' \\n[2023-01-19T16:39:06.381402][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\\n[2023-01-19T16:39:06.5913864Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_c4fabe37-8835-473c-8647-824413410f00_0' \\n[2023-01-19T16:39:06.6712993Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_c4fabe37-8835-473c-8647-824413410f00_1' \\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.48.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: HD_c4fabe37-8835-473c-8647-824413410f00\n",
      "Web View: https://ml.azure.com/runs/HD_c4fabe37-8835-473c-8647-824413410f00?wsid=/subscriptions/764a3c74-3d65-4a9c-bc90-661b00666572/resourcegroups/rg-con-fer-cvrail-euwe-dev/workspaces/aml-con-fer-cvrail-euwe-dev&tid=a9a8e375-fac1-4ec2-820a-cfb6eb5cf01b\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "[2023-01-19T16:39:05.507582][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\n",
      "[2023-01-19T16:39:06.4306644Z][SCHEDULER][INFO]Scheduling job, id='HD_c4fabe37-8835-473c-8647-824413410f00_1' \n",
      "[2023-01-19T16:39:06.3481073Z][SCHEDULER][INFO]Scheduling job, id='HD_c4fabe37-8835-473c-8647-824413410f00_0' \n",
      "[2023-01-19T16:39:06.381402][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\n",
      "[2023-01-19T16:39:06.5913864Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_c4fabe37-8835-473c-8647-824413410f00_0' \n",
      "[2023-01-19T16:39:06.6712993Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_c4fabe37-8835-473c-8647-824413410f00_1' \n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "import pandas as pd\n",
    "\n",
    "# Construct the pipeline\n",
    "#pipeline_steps = [pidnet_training_step]\n",
    "#pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "#print(\"Pipeline is built.\")\n",
    "\n",
    "# Define experiment tags\n",
    "# Incluse some tags register in the experiment\n",
    "output_tags = {}\n",
    "output_tags['Area']='Computer Vision, Railway Inspection'\n",
    "output_tags['Target'] = 'Railway Semantic Segmentation'\n",
    "output_tags['Subscription ID'] = ws.subscription_id\n",
    "output_tags['Workspace'] = ws.name\n",
    "output_tags['Resource Group'] = ws.resource_group\n",
    "output_tags['Location'] = ws.location\n",
    "output_tags['ADL'] = ws.tags['ADL']\n",
    "output_tags['Company'] = ws.tags['Company']\n",
    "output_tags['DevOps'] = ws.tags['DevOps']\n",
    "output_tags['Environment'] = ws.tags['Environment']\n",
    "output_tags['OU'] = ws.tags['OU']\n",
    "output_tags['Project'] = ws.tags['Project']\n",
    "output_tags['Responsible'] = ws.tags['Responsible']\n",
    "output_tags['area'] = \"railway inspection\"\n",
    "output_tags['type'] = \"railway semantic segmentation PIDNet model training\"\n",
    "output_tags['Project'] = \"Railway Inspection\"\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "outputDf = pd.DataFrame(data = output_tags, index = [''])\n",
    "print(outputDf.T)\n",
    "\n",
    "# Create an experiment, set some tags and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'railway-pidnet-model-training')\n",
    "experiment.set_tags(output_tags)\n",
    "pipeline_run = experiment.submit(config=hyperdrive)\n",
    "#pipeline_run = experiment.submit(pipeline, regenerate_outputs = True)\n",
    "#pipeline_run = experiment.submit(pipeline)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
